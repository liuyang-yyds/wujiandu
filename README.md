# UMSF-Net: Unsupervised Multi-Source Fusion Network for Land Cover Classification

<p align="center">
  <img src="assets/framework.png" width="800"/>
</p>

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-orange.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This is the official PyTorch implementation of **"Unsupervised Land Cover Classification by Fusing SAR and Multispectral Optical Images via Cross-Modal Contrastive Learning"**.

## ğŸ“‹ Table of Contents

- [Highlights](#-highlights)
- [Network Architecture](#-network-architecture)
- [Installation](#-installation)
- [Dataset](#-dataset)
- [Training](#-training)
- [Evaluation](#-evaluation)
- [Results](#-results)
- [Visualization](#-visualization)
- [Citation](#-citation)
- [Acknowledgements](#-acknowledgements)

## âœ¨ Highlights

- **Dual-Branch Feature Extraction**: Specialized encoders for optical and SAR images with domain-specific preprocessing
- **Cross-Modal Contrastive Learning**: Deep alignment of optical-SAR features using InfoNCE loss with momentum contrast
- **Attention-based Fusion**: Adaptive multi-head cross-attention mechanism for modality fusion
- **End-to-End Unsupervised Framework**: No manual annotations required for land cover classification

<p align="center">
  <img src="assets/highlights.png" width="600"/>
</p>

## ğŸ— Network Architecture

### Overall Framework

```
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚           UMSF-Net Framework            â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                                          â”‚
                    â–¼                                                          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      Optical Branch           â”‚                      â”‚        SAR Branch             â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚   ResNet50 Backbone     â”‚  â”‚                      â”‚  â”‚  Despeckling + ResNet50 â”‚  â”‚
    â”‚  â”‚  (ImageNet pretrained)  â”‚  â”‚                      â”‚  â”‚  (Modified for 1-ch)    â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                                                      â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   Cross-Modal Attention       â”‚
                          â”‚        Fusion Module          â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚                                           â”‚
                   â–¼                                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      Projection Head          â”‚           â”‚      Clustering Head          â”‚
    â”‚   (Contrastive Learning)      â”‚           â”‚   (Unsupervised Classification)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

| Module | Description | Output Dim |
|--------|-------------|------------|
| Optical Encoder | ResNet50 with ImageNet pretrained weights | 2048 |
| SAR Encoder | Learnable despeckling + modified ResNet50 | 2048 |
| Attention Fusion | Multi-head cross-attention (8 heads) | 2048 |
| Projection Head | MLP (2048 â†’ 2048 â†’ 256) | 256 |
| Clustering Head | MLP (2048 â†’ 512 â†’ K) | K classes |

## ğŸ”§ Installation

### Requirements

- Python >= 3.10
- PyTorch >= 2.0.0
- CUDA >= 12.0

### Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/UMSF-Net.git
cd UMSF-Net

# Create conda environment
conda create -n umsf python=3.10 -y
conda activate umsf

# Install PyTorch (adjust according to your CUDA version)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Install dependencies
pip install -r requirements.txt
```

## ğŸ“Š Dataset

We use the **WHU-OPT-SAR** dataset released by Wuhan University.

### Download

Download the dataset from: [WHU-OPT-SAR Dataset](https://github.com/AmberHen/WHU-OPT-SAR-dataset)

### Data Structure

```
data/
â”œâ”€â”€ WHU-OPT-SAR/
â”‚   â”œâ”€â”€ optical/
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ val/
â”‚   â”‚   â””â”€â”€ test/
â”‚   â”œâ”€â”€ sar/
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ val/
â”‚   â”‚   â””â”€â”€ test/
â”‚   â””â”€â”€ labels/
â”‚       â”œâ”€â”€ train/
â”‚       â”œâ”€â”€ val/
â”‚       â””â”€â”€ test/
```

### Data Preprocessing

```bash
# Prepare dataset (crop patches, split train/val/test)
python scripts/prepare_data.py \
    --data_root /path/to/WHU-OPT-SAR \
    --output_dir ./data/processed \
    --patch_size 256 \
    --overlap 64
```

### Class Definition

| Class ID | Class Name | Color |
|----------|------------|-------|
| 0 | Farmland | ğŸŸ© Green |
| 1 | City | ğŸŸ¥ Red |
| 2 | Village | ğŸŸ§ Orange |
| 3 | Water | ğŸŸ¦ Blue |
| 4 | Forest | ğŸŒ² Dark Green |
| 5 | Road | â¬œ White |
| 6 | Others | â¬› Gray |

## ğŸš€ Training

### Single GPU Training

```bash
python train.py \
    --config configs/umsf_whu.yaml \
    --data_root ./data/processed \
    --output_dir ./outputs
```

### Multi-GPU Training (DDP)

```bash
torchrun --nproc_per_node=4 train.py \
    --config configs/umsf_whu.yaml \
    --data_root ./data/processed \
    --output_dir ./outputs \
    --distributed
```

### Key Training Arguments

| Argument | Default | Description |
|----------|---------|-------------|
| `--config` | Required | Path to config YAML file |
| `--data_root` | Required | Path to processed dataset |
| `--output_dir` | `./outputs` | Output directory |
| `--epochs` | 200 | Total training epochs |
| `--batch_size` | 32 | Batch size per GPU |
| `--lr` | 0.03 | Initial learning rate |
| `--num_classes` | 7 | Number of land cover classes |
| `--resume` | None | Resume from checkpoint |

### Training Configuration

```yaml
# configs/umsf_whu.yaml
model:
  backbone: resnet50
  pretrained: true
  feature_dim: 2048
  projection_dim: 256
  num_classes: 7

training:
  epochs: 200
  batch_size: 32
  optimizer:
    type: SGD
    lr: 0.03
    momentum: 0.9
    weight_decay: 0.0001
  scheduler:
    type: cosine
    warmup_epochs: 10
  contrastive:
    temperature: 0.07
    queue_size: 65536
    momentum: 0.999
  loss_weights:
    contrastive_intra: 1.0
    contrastive_cross: 1.0
    clustering: 0.5
    consistency: 0.5
```

## ğŸ“ˆ Evaluation

### Run Evaluation

```bash
python evaluate.py \
    --config configs/umsf_whu.yaml \
    --checkpoint ./checkpoints/best_model.pth \
    --data_root ./data/processed \
    --output_dir ./results
```

### Evaluation Metrics

- **ACC**: Clustering Accuracy
- **NMI**: Normalized Mutual Information  
- **ARI**: Adjusted Rand Index
- **F1**: Macro F1 Score

## ğŸ“Š Results

### Comparison with State-of-the-art Methods

| Method | ACC (%) | NMI | ARI | F1 |
|--------|---------|-----|-----|-----|
| K-Means | 42.3 | 0.21 | 0.12 | 0.38 |
| DeepCluster | 55.6 | 0.36 | 0.26 | 0.51 |
| SwAV | 61.2 | 0.42 | 0.31 | 0.57 |
| SCAN | 63.5 | 0.44 | 0.33 | 0.59 |
| **UMSF-Net (Ours)** | **72.8** | **0.53** | **0.42** | **0.68** |

### Ablation Study

| Variant | ACC (%) | NMI |
|---------|---------|-----|
| Optical Only | 58.3 | 0.38 |
| SAR Only | 45.7 | 0.28 |
| Concat Fusion | 65.4 | 0.45 |
| Add Fusion | 66.8 | 0.47 |
| **Attention Fusion** | **72.8** | **0.53** |

## ğŸ¨ Visualization

### Generate Visualizations

```bash
python visualization/visualize.py \
    --checkpoint ./checkpoints/best_model.pth \
    --data_root ./data/processed \
    --output_dir ./vis_results \
    --num_samples 16
```

### t-SNE Feature Visualization

<p align="center">
  <img src="assets/tsne.png" width="400"/>
</p>

### Classification Results

<p align="center">
  <img src="assets/results.png" width="700"/>
</p>

## ğŸ“ Project Structure

```
UMSF-Net/
â”œâ”€â”€ configs/                  # Configuration files
â”‚   â”œâ”€â”€ umsf_whu.yaml
â”‚   â””â”€â”€ default.yaml
â”œâ”€â”€ datasets/                 # Dataset and dataloader
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ whu_opt_sar.py
â”‚   â”œâ”€â”€ transforms.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ models/                   # Model architectures
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ umsf_net.py          # Main network
â”‚   â”œâ”€â”€ encoders.py          # Optical & SAR encoders
â”‚   â”œâ”€â”€ fusion.py            # Attention fusion module
â”‚   â”œâ”€â”€ heads.py             # Projection & clustering heads
â”‚   â””â”€â”€ losses.py            # Loss functions
â”œâ”€â”€ utils/                    # Utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics.py           # Evaluation metrics
â”‚   â”œâ”€â”€ logger.py            # Logging utilities
â”‚   â””â”€â”€ misc.py              # Miscellaneous utilities
â”œâ”€â”€ scripts/                  # Helper scripts
â”‚   â”œâ”€â”€ prepare_data.py
â”‚   â””â”€â”€ download_data.sh
â”œâ”€â”€ visualization/            # Visualization tools
â”‚   â”œâ”€â”€ visualize.py
â”‚   â””â”€â”€ plot_utils.py
â”œâ”€â”€ train.py                  # Training script
â”œâ”€â”€ evaluate.py               # Evaluation script
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

## ğŸ“ Citation

If you find this work useful, please consider citing:

```bibtex
@article{yourname2026umsf,
  title={Unsupervised Land Cover Classification by Fusing SAR and Multispectral Optical Images via Cross-Modal Contrastive Learning},
  author={Your Name},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  year={2026}
}
```

## ğŸ™ Acknowledgements

- [WHU-OPT-SAR Dataset](https://github.com/AmberHen/WHU-OPT-SAR-dataset) for providing the optical-SAR paired dataset
- [MoCo](https://github.com/facebookresearch/moco) for the momentum contrast framework
- [SwAV](https://github.com/facebookresearch/swav) for the clustering-based contrastive learning

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“§ Contact

If you have any questions, please feel free to open an issue or contact us at [your-email@example.com](mailto:your-email@example.com).
